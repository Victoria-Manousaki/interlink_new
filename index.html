<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>InterLinK</title>

  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.1.3/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Poppins:400,600,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="css/responsive.css" rel="stylesheet" />
</head>

<body>

  <div class="hero_area">
    <!-- header section strats -->
    <header class="header_section">
      <div class="container">
        <nav class="navbar navbar-expand-lg custom_nav-container ">
          <a class="navbar-brand" href="index.html">
            <img src="images/interlink_logo.png" alt="">
       
          </a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="s-1"> </span>
            <span class="s-2"> </span>
            <span class="s-3"> </span>
          </button>

          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="d-flex ml-auto flex-column flex-lg-row align-items-center">
              <ul class="navbar-nav  ">

                <li class="nav-item">
                  <a class="nav-link" href="#Concept"> Project Concept</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="#acks"> Acknowledements </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="#collab"> Collaborators </a>
                </li>
				<li class="nav-item">
                  <a class="nav-link" href="#events"> Events/Publications </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="#contact">Contact </a>
                </li>
              </ul>
            </div>
          </div>
        </nav>
      </div>
    </header>
    <!-- end header section -->
    <!-- slider section -->
    <section class=" slider_section ">
      <div class="container">
        <div class="row">
          <div class="col-md-6 ">
            <div class="detail_box">
              <h1>
                InteRlinK <br>
                Project <br>
              </h1>
              <p>
                Visual Recognition and Anticipation of Human-Object Interactions using Deep Learning, Knowledge Graphs and Reasoning<br>
				<br>
				Principal Investigator: Konstantinos Papoutsakis<br>
				<br>
				Department of Management, Science and Technology, <br> Hellenic Mediterranean University
              </p>
              <a href="#contact" class="">
                Contact Us
            </div>
          </div>
          <div class="col-lg-5 col-md-6 offset-lg-1">
            <div class="img_content">
              <div class="img_container">
                <div id="carouselExampleControls" class="carousel slide" data-ride="carousel">
                  <div class="carousel-inner">
                    <div class="carousel-item active">
                      <div class="img-box">
                        <img src="images/INTERLINK.png" alt="">
                      </div>
                    </div>
                    <div class="carousel-item">
                      <div class="img-box">
                        <img src="images/datalab.gif" alt="">
                      </div>
                    </div>
                    <div class="carousel-item">
                      <div class="img-box">
                        <img src="images/hmu.png" alt="">
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <a class="carousel-control-prev" href="#carouselExampleControls" role="button" data-slide="prev">
                <span class="sr-only">Previous</span>
              </a>
              <a class="carousel-control-next" href="#carouselExampleControls" role="button" data-slide="next">
                <span class="sr-only">Next</span>
              </a>
            </div>

          </div>
        </div>
      </div>
    </section>
    <!-- end slider section -->
  </div>


  <!-- service section -->
  <section class="service_section layout_padding" id="Concept">
    <div class="container">
      <div class="heading_container">
        <h2>
          Project Concept/Goals
        </h2>
       
      </div>

      <div class="service_container">
        <div class="box active">
          <div class="img-box">
            <img src="images/s1.png" class="img1" alt="">
          </div>
          <div class="detail-box">
            <h5>
              Goal:
            </h5>
            <p>
              In InterLinK, we envision a novel AI-based framework that will empower smart agents and robotic systems to visually 
			  recognize and anticipate high-level semantics of Human-Object(s) Interactions (HOI) based on deep neural networks, knowledge graphs and visual reasoning.
            </p>
          </div>
        </div>
        <div class="box">
          <div class="img-box">
            <img src="images/s2.png" class="img1" alt="">
          </div>
          <div class="detail-box">
            <h5>
              RO1:
            </h5>
            <p>
              The development of a novel, semantically-enriched, hierarchical representation of HOI comprising three main components: (i) graph-based modelling of the semantics, 
			  appearance and dynamics of human or hand(s) and object(s) at multiple levels of abstraction, adaptive to the scale of the available observations, (ii) learning the 
			  temporal semantic structure of HOI as spatio-temporal scene graphs and (iii) reasoning about action-objects relationships using semantic information based on knowledge graphs.
            </p>
          </div>
        </div>
        <div class="box">
          <div class="img-box">
            <img src="images/s3.png" class="img1" alt="">
          </div>
          <div class="detail-box">
            <h5>
              RO2:
            </h5>
            <p>
              The acquisition of a new dataset of visual and motion data demonstrating fine-grained HOI during daily living activities (ADL) using household objects, at multiple observation scales. Rich annotations will be provided with respect to the semantics of HOI at multiple-levels of abstraction, 2D and 3D ground truth data of the geometry and pose of humans, hands and objects during manipulation tasks.
            </p>
          </div>
        </div>
        <div class="box ">
          <div class="img-box">
            <img src="images/s4.png" class="img1" alt="">
          </div>
          <div class="detail-box">
            <h5>
              RO3:
            </h5>
            <p>
              The development of a novel visual-semantic method for recognition of fine-grained HOI and reasoning in long videos using Deep Learning (DL) and Knowledge Graphs (KG).
            </p>
          </div>
        </div>
        <div class="box">
          <div class="img-box">
            <img src="images/s5.png" class="img1" alt="">
          </div>
          <div class="detail-box">
            <h5>
              RO4: 
            </h5>
            <p>
              The development of a novel vision-based method for short- and long-term prediction and anticipation of fine-grained HOI in long videos using Deep Learning and high-level semantics based on KG.
            </p>
          </div>
        </div>
      </div>
     
    </div>
  </section>
  <!-- end service section -->

  <!-- about section -->
  <section class="about_section layout_padding" id="acks">
    <div class="container">
      <div class="row">
        <div class="col-md-6">
          <div class="detail-box">
            <div class="heading_container">
              <h2>
                Acknowledements
              </h2>
             
            </div>
            <p>
              The research project is supported by the Hellenic Foundation for Research and Innovation (H.F.R.I.) under the 
			  “3rd Call for H.F.R.I. Research Projects to support Post-Doctoral Researchers” (Project Number: 07678). <br>
				2023 – 2025<br>
				<br>
				Ιn collaboration with the Institute of Computer Science, Foundation of Research and Technology- Hellas (FORTH), 
				Computer Vision & Robotics Lab (CVRL) and the Information System Lab (ISL).
            </p>
            
          </div>
        </div>
        <div class="col-md-6">
          <div class="img_container">
            <div class="img-box b1">
              <img src="images/elidek.png" alt="" />
            </div>
            <div class="img-box b2">
              <img src="images/ite_logo.jpg" alt="" />
            </div>
          </div>

        </div>

      </div>
    </div>
  </section>

  <!-- end about section -->




  <!-- blog section -->

  <section class="blog_section layout_padding" id="collab">
    <div class="container">
      <div class="heading_container">
        <h2>
          Principal Investigator
        </h2>
     
		<br><br>
      </div>
	  
      <div class="row">
        <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/papoutsa.jpg" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Konstantinos Papoutsakis, D.Eng., M.Sc., Ph.D.
              </h5>
              <p>
                Konstantinos Papoutsakis is a postdoctoral researcher at the Department of Management, Science and Technology at the Hellenic Mediterranean University in Crete, 
				and is also affiliated to the Computer Vision and Robotics Laboratory of the Institute of Computer Science at FORTH. He earned his diploma in Computer Engineering 
				and Informatics from the University of Patras in Greece in 2007, his M.Sc. and PhD degrees in Computer Science from the University of Crete in 2010 and 2019, respectively. 
				His main research interests fall in the areas of computer vision, machine learning and visual perception for robotics with emphasis on human motion analysis, video segmentation, 
				action segmentation and recognition and human-computer and human-robot interaction. He has also been actively involved in EU funded projects as a Computer Vision Engineer and 
				during his postgraduate studies. For more information: 
				<b></b> <a href="http://users.ics.forth.gr/~papoutsa/">http://users.ics.forth.gr/~papoutsa/ </a>
              </p>
            </div>
          </div>
        </div>
	  </div>	
	</div>
	<br><br>
	<div class="container">
      <div class="heading_container">
        <h2>
          Research Team
        </h2>

      </div>
	  
	<div class="row">
        <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/fillipos.png" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Fillipos Gouidis
              </h5>
              <p>
                Filippos Gouidis is a PhD candidate at the university of Crete. He received a Diploma in Civil Engineering 
				from the aristotelian university of Thessaloniki and a BSc in Computer Science from the university of Crete. 
				He holds a MSc in Computer Science and Engineering from the university of Crete and a MSc in Cognitive Sciences 
				from the university of Crete. His research interests focus on image classification and detection, knowledge representation and zero-shot learning.
              </p>
            </div>
          </div>
        </div>
        <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/victoria.jpg" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Victoria Manousaki
              </h5>
              <p>
                Victoria Manousaki is a Postdoctoral Researcher at the Department of Management Science and Technology of HMU also collaborating with
				the Computational Vision and Robotics Laboratory (CVRL) at FORTH. She receined her Ph.D. from the Computer Science department of the University of Crete.
				Her research interests lie in the topics of action/activity prediction, anticipation and recognition in human-object interactions.  
              </p>
            </div>
          </div>
        </div>
      </div>
      </div>
 
  </section>

  <!-- end blog section -->









  <!-- blog section -->

  <section class="blog_section layout_padding">
    <div class="container">
      <div class="heading_container">
        <h2>
          Advisory Board
        </h2>
        
      </div>
      <div class="row">
        <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/Costas-Panagiotakis.jpg" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Constantinos Panagiotakis 
              </h5>
              <p>
                Constantinos Panagiotakis is an Associate Professor and Head of the DMST at the HMU and Director of DataLab, host of the InterLinK project. 
				His research interests span the areas of image and video analysis, data modelling, machine learning, 3D animation, signal processing, multimedia 
				and pattern recognition. He has published more than 90 articles in international conferences and journals.
              </p>
            </div>
          </div>
        </div>
        <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/plexousakis.jpg" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Dimitris Plexousakis 
              </h5>
              <p>
                Dimitris Plexousakis is a Professor at the Computer Science Department (CSD) of the University of Crete (UoC), Director of FORTH-ICS and head of 
				the Information Systems Laboratory at FORTH-ICS. His research interests span the areas of Knowledge Representation, Knowledge Base Design, Formal 
				reasoning systems, Semantic Web and more. He has published over 180 articles in international conferences and journals. He has extensive experience 
				in the scientific coordination of national and European research projects. 
              </p>
            </div>
          </div>
        </div>
		
		  <div class="col-md-6">
          <div class="box">
            <div class="img-box">
              <img src="images/patkos.jpeg" alt="">
            </div>
            <div class="detail-box">
              <h5>
                Theodore Patkos 
              </h5>
              <p>
                Theodore Patkos is a Principal Researcher (Grade B) at FORTH-ICS. His research activities revolve around the fields of knowledge representation and 
				non-monotonic reasoning, contextual and common-sense reasoning, multi-agent and cognitive systems, argumentation and formal representation models for 
				the Semantic Web. He has served as PI and as a researcher in national and European projects, has co-authored more than 55 scientific papers in peer-reviewed conferences and journals, 
				including IJCAI, KR, TPLP, AAMAS and LPAR.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- end blog section -->





 <!-- about section -->
  <section class="events_section layout_padding" id="events">
    <div class="container">
      <div class="row">
          <div class="detail-box2">
            <div class="heading_container">
              <h2>
                Events <br><br>
              </h2>
              
            </div>
            <p>
              Presentation of InterLinK project at the <a href="https://ysc.actcognitive.org/">International Young Scientists Conference </a> for young researchers and professionals 
			in computational science, Artificial Intelligence, Big Data and Machine Learning. ITMO, October 2023 in Abu Dhabi. <br>
			<br>
			<b>Link:</b> <a href="https://github.com/itmo-ai/YSC-2023-Papers ">https://github.com/itmo-ai/YSC-2023-Papers  </a>
            </p>
            
          </div>
      </div>
    </div>
  </section>

  <!-- end about section -->





 <!-- about section -->
  <section class="events_section layout_padding">
    <div class="container">
      <div class="row">
        
          <div class="detail-box2">
            <div class="heading_container">
              <h2>
                Publications <br><br>
              </h2>
              
            </div>
           <p class="mb-0">Victoria Manousaki, Konstantinos Bacharidis, Konstantinos Papoutsakis and Antonis Argyros, <b>"VLMAH: Visual-Linguistic Modeling of Action History for Effective Action Anticipation"</b>, 
			In IEEE International Conference on Computer Vision Workshops (ACVR 2023), Paris, France, October 2023. </p>
			<br>
			<p> <b>Absract:</b> Although existing methods for action anticipation have
					shown considerably improved performance on the predictability of future events in videos, the way they exploit information related to past actions is constrained by time duration and encoding complexity. This paper addresses the
					task of action anticipation by taking into consideration the
					history of all executed actions throughout long, procedural activities. A novel approach noted as Visual-Linguistic
					Modeling of Action History (VLMAH) is proposed that fuses
					the immediate past in the form of visual features as well as
					the distant past based on a cost-effective form of linguistic constructs (semantic labels of the nouns, verbs, or actions). Our approach generates accurate near-future action
					predictions during procedural activities by leveraging information on the long- and short-term past. Extensive experimental evaluation was conducted on three challenging
					video datasets containing procedural activities, namely the
					Meccano, the Assembly-101, and the 50Salads. The results
					confirm that using long-term action history improves action
					anticipation and enhances the SOTA Top-1 accuracy.</p>
			<p><b>Paper:</b> <a href="https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Manousaki_VLMAH_Visual-Linguistic_Modeling_of_Action_History_for_Effective_Action_Anticipation_ICCVW_2023_paper.pdf ">Read online  </a>
			</p>
            
			
			<br>
			<p class="mb-0">Filippos Gouidis, Konstantinos Papoutsakis, Theodore Patkos, Antonis Argyros and Dimitris Plexousakis, 
			<b>"Exploring the Impact of Knowledge Graphs on Zero-Shot Visual Object State Classification"</b>, 
			In International Conference on Computer Vision Theory and Applications 2024, Rome, Italy. </p>
			<br>
			<p> <b>Absract:</b> In this work, we explore the potential of Knowledge Graphs (KGs) towards an effective Zero-Shot Learning
				(ZSL) approach for Object State Classification (OSC) in images. For this problem, the performance of tradi-
				tional supervised learning methods is hindered mainly by data scarcity, as they attempt to encode the highly
				varying visual features of a multitude of combinations of object state and object type classes (e.g. open bottle,
				folded newspaper). The ZSL paradigm does indicate a promising alternative to enable the classification of
				object state classes by leveraging structured semantic descriptions acquired by external commonsense knowl-
				edge sources. We formulate an effective ZS-OSC scheme by employing a Transformer-based Graph Neural
				Network model and a pre-trained CNN classifier. We also investigate best practices for both the construction
				and integration of visually-grounded common-sense information based on KGs. An extensive experimental
				evaluation is reported using 4 related image datasets, 5 different knowledge repositories and 30 KGs that are
				constructed semi-automatically via querying known object state classes to retrieve contextual information at
				different node depths. The performance of vision-language models for ZS-OSC is also assessed. Overall, the
				obtained results suggest performance improvement for ZS-OSC models on all datasets, while both the size of
				a KG and the sources utilized for their construction are important for task performance.</p>
			<p><b>Paper:</b> <a href="">Read online  </a>
			</p>
			
			
			<br>
			<p class="mb-0">F. Gouidis, K. Papantoniou, K. Papoutsakis, T. Patkos, A.A. Argyros and D. Plexousakis, 
			<b>"Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification"</b>, 
			In AAAI 2024 Spring Symposium on Empowering Machine Learning and Large Language Models with Domain and Commonsense Knowledge, (AAAI-MAKE), (to appear), Stanford University, USA, March 2024. </p>
			<br>
			<p> <b>Absract:</b> </p>
			<p><b>Paper:</b> <a href="">Read online  </a>
			</p>
          </div>
       
        

      </div>
    </div>
  </section>

  <!-- end about section -->




  <!-- contact section -->

  <section class="contact_section layout_padding" id="contact">
    <div class="container ">
      <div class="heading_container">
        <h2>
          Contact Us
        </h2>
        
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-md-6">
          <form action="">
            <div>
              <input type="text" placeholder="Name" />
            </div>
            <div>
              <input type="email" placeholder="Email" />
            </div>
            <div>
              <input type="text" placeholder="Phone Number" />
            </div>
            <div>
              <input type="text" class="message-box" placeholder="Message" />
            </div>
            <div class="d-flex ">
              <button>
                SEND
              </button>
            </div>
          </form>
        </div>
        <div class="col-md-6">
          <div class="map_container">
            <div class="map-responsive">
              <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3260.4750834756173!2d25.6568946!3d35.1946335!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x1490819fac18d4bb%3A0x43db250415c42a8e!2zzqTOvM6uzrzOsSDOlM65zr_Ouc66zrfPhM65zrrOrs-CIM6Vz4DOuc-Dz4TOrs68zrfPgiDOus6xzrkgzqTOtc-Hzr3Ov867zr_Os86vzrHPgiB8IM6VzrvOu863zr3Ouc66z4wgzpzOtc-Dzr_Os861zrnOsc66z4wgzqDOsc69zrXPgM65z4PPhM6uzrzOuc6_!5e0!3m2!1sel!2sgr!4v1702045102163!5m2!1sel!2sgr" width="600" height="300" frameborder="0" style="border:0; width: 100%; height:100%" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- end contact section -->


  <!-- info section -->

  <section class="info_section layout_padding">
    <div class="container">
      <div class="info_contact">
        <div class="row">
          <div class="col-md-4">
            <a href="">
              <img src="images/location-white.png" alt="">
              <span>
                Department of Management, Science and Technology, Hellenic Mediterranean University
              </span>
            </a>
          </div>
          <div class="col-md-4">
            <a href="">
              <img src="images/telephone-white.png" alt="">
              <span>
                Call : +30-28410-91103
              </span>
            </a>
          </div>
          <div class="col-md-4">
            <a href="">
              <img src="images/envelope-white.png" alt="">
              <span>
                kpapoutsakis@hmu.gr
              </span>
            </a>
          </div>
        </div>
      </div>


    </div>
  </section>

  <!-- end info section -->

  <!-- footer section -->
  <footer class="container-fluid footer_section">
    <div class="container">
      <div class="row">
        <div class="col-lg-7 col-md-9 mx-auto">
          <p>
            &copy; 2023 All Rights Reserved By
            <a href="https://html.design/">Free Html Templates</a>
          </p>
        </div>
      </div>
    </div>
  </footer>
  <!-- footer section -->


  <script src="js/jquery-3.4.1.min.js"></script>
  <script src="js/bootstrap.js"></script>

</body>

</html>